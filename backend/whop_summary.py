import datetime
import pytz
import os
import time
from loguru import logger
from utils import history_list_to_text, get_knowledge_macro_prompt, get_timeline_views_prompt, get_metadata_prompt, get_response, save_summary_to_db, get_trading_window_cn, get_trading_window_cn_offset
from utils.local_secrets import supabase_url, supabase_key
from supabase import create_client, Client
import json
import re

# åˆå§‹åŒ– Supabase
try:
    supabase: Client = create_client(supabase_url, supabase_key)
except Exception as e:
    logger.error(f"Supabase åˆå§‹åŒ–å¤±è´¥: {e}")
    supabase = None


def _iso_to_ms(iso: str) -> int:
    if iso:
        try:
            dt = datetime.datetime.fromisoformat(str(iso).replace('Z', '+00:00'))
            return int(dt.timestamp() * 1000)
        except:
            return 0
    return 0

def fetch_vip_data(start_ms: int, end_ms: int):
    """
    ä» whop_vip_posts è¡¨è·å–æŒ‡å®šæ—¶é—´çª—å£å†…çš„ VIP æ•°æ®ï¼Œå¹¶é‡å»ºä¸Šä¸‹æ–‡ã€‚
    æ³¨æ„ï¼šwhop_vip_posts å·²åŒ…å« whop_posts å’Œ whop_universal_posts ä¸­çš„ VIP å†…å®¹ã€‚
    """
    if not supabase:
        return [], {}

    try:
        # 1. æŸ¥è¯¢ VIP å¸–å­ (whop_vip_posts)
        start_iso = datetime.datetime.fromtimestamp(start_ms / 1000.0, tz=datetime.timezone.utc).isoformat()
        end_iso = datetime.datetime.fromtimestamp(end_ms / 1000.0, tz=datetime.timezone.utc).isoformat()
        
        query = supabase.table("whop_vip_posts").select("*").order("posted_at", desc=True)
        query = query.gt("posted_at", start_iso).lt("posted_at", end_iso)
            
        resp = query.execute()
        vip_rows = resp.data
        
        if not vip_rows:
            return [], {}
            
        # 2. æ”¶é›†éœ€è¦è¡¥å……çš„ Parent ID å’Œ User ID
        parent_ids = set()
        user_ids = set()
        
        for row in vip_rows:
            if row.get('user_id'):
                user_ids.add(row['user_id'])
            if row.get('reply_to_post_id'):
                parent_ids.add(row['reply_to_post_id'])
                
        # 3. è·å– Parent Posts (Context)
        parent_map = {}
        if parent_ids:
            p_resp = supabase.table("whop_posts").select("*").in_("id", list(parent_ids)).execute()
            for p in p_resp.data:
                parent_map[p['id']] = p
                if p.get('user_id'):
                    user_ids.add(p['user_id'])
                    
        # 4. è·å–ç”¨æˆ·ä¿¡æ¯
        user_map = {}  # id -> username/name
        if user_ids:
            u_resp = supabase.table("whop_users").select("id, username, display_name").in_("id", list(user_ids)).execute()
            for u in u_resp.data:
                name = u.get('display_name') or u.get('username') or "Unknown"
                user_map[u['id']] = name

        # 5. æ ¼å¼åŒ–ä¸º history_list_to_text æ‰€éœ€çš„ç»“æ„
        formatted_items = []
        
        for row in vip_rows:
            item = {
                'id': row['id'],
                'userId': row['user_id'],
                'content': row['content'],
                'createdAt': _iso_to_ms(row['posted_at']),
                'attachments': row.get('attachments') or [],
                'isPosterAdmin': False,
                'is_vip_related': True,
            }
            
            u_name = user_map.get(row['user_id'], "Unknown")
            item['user'] = {'username': u_name, 'name': u_name}
            
            # è¡¥å…… Reply Context
            reply_id = row.get('reply_to_post_id')
            if reply_id and reply_id in parent_map:
                parent = parent_map[reply_id]
                p_user_id = parent.get('user_id')
                p_name = user_map.get(p_user_id, "Unknown")
                
                item['replyingToPost'] = {
                    'id': reply_id,
                    'content': parent.get('content'),
                    'user': {
                        'username': p_name,
                        'name': p_name
                    }
                }
            
            formatted_items.append(item)
            
        return formatted_items, user_map

    except Exception as e:
        logger.error(f"Fetch VIP Data Failed: {e}")
        return [], {}

def get_last_summary_time() -> int:
    """
    è·å–æœ€è¿‘ä¸€æ¬¡æ€»ç»“çš„æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰- ä»æ•°æ®åº“æŸ¥è¯¢
    """
    if not supabase:
        return 0
    
    try:
        # æŸ¥è¯¢æœ€æ–°çš„ä¸€æ¡è®°å½•
        resp = supabase.table("whop_summaries").select("created_at").order("created_at", desc=True).limit(1).execute()
        if resp.data:
            iso_time = resp.data[0]['created_at']
            # ISO -> æ¯«ç§’
            dt = datetime.datetime.fromisoformat(iso_time.replace('Z', '+00:00'))
            return int(dt.timestamp() * 1000)
    except Exception as e:
        logger.error(f"æŸ¥è¯¢ä¸Šæ¬¡æ€»ç»“æ—¶é—´å¤±è´¥: {e}")
        
    return 0

def summary_run(start_ms: int, end_ms: int, title: str, description: str) -> bool:
    """
    ç”Ÿæˆå¹¶ä¿å­˜æ€»ç»“ (åŒ Agent æ¨¡å¼ + åŠ¨æ€å…ƒæ•°æ®)
    
    Returns:
        bool: True è¡¨ç¤ºæˆåŠŸç”Ÿæˆæ€»ç»“ï¼ŒFalse è¡¨ç¤ºè·³è¿‡ï¼ˆæ— æœ‰æ•ˆæ•°æ®ï¼‰
    """
    logger.info(f"å¼€å§‹ç”Ÿæˆæ€»ç»“: {title} ({description})")
    
    # æ‹‰å–æŒ‡å®šçª—å£å†…çš„æ‰€æœ‰æ•°æ®
    history_items, username_dict = fetch_vip_data(start_ms=start_ms, end_ms=end_ms)
    
    # å‰ç½®æ£€æŸ¥ï¼šå¦‚æœæ²¡æœ‰ VIP èŠå¤©å†…å®¹ï¼Œè·³è¿‡ LLM è°ƒç”¨
    if not history_items:
        # æ£€æŸ¥æ˜¯å¦ä¸ºå‘¨æœ« (å‘¨å…­=5, å‘¨æ—¥=6)
        # æ³¨æ„ï¼šstart_ms æ˜¯åŒ—äº¬æ—¶é—´ 09:00
        tz_cn = pytz.timezone("Asia/Shanghai")
        start_dt = datetime.datetime.fromtimestamp(start_ms / 1000.0, tz=tz_cn)
        weekday = start_dt.weekday()
        
        if weekday in [5, 6]:
            logger.info(f"å½“å‰çª—å£èµ·å§‹äºå‘¨{'å…­' if weekday == 5 else 'æ—¥'} ({start_dt.strftime('%Y-%m-%d')})ï¼Œé€šå¸¸æ— ç¾è‚¡äº¤æ˜“æ•°æ®ï¼Œæ­£å¸¸è·³è¿‡ã€‚")
        else:
            logger.warning("è¯¥æ—¶é—´çª—å£å†…æ²¡æœ‰ VIP èŠå¤©å†…å®¹ï¼Œè·³è¿‡ LLM æ€»ç»“ç”Ÿæˆä»¥èŠ‚çœ Tokenã€‚")
        return False
    
    vip_username = "xiaozhaolucky"
    # å…¨é‡å›é¡¾æ¨¡å¼ï¼Œlast_summary_time è®¾ä¸º 0
    big_text = history_list_to_text(history_items, username_dict, last_summary_time=0, vip_username=vip_username)
    
    # å†…å®¹æœ‰æ•ˆæ€§æ£€æŸ¥ï¼šå¦‚æœè½¬æ¢åçš„æ–‡æœ¬è¿‡çŸ­ï¼Œè¯´æ˜æ²¡æœ‰æœ‰æ„ä¹‰çš„å†…å®¹
    if not big_text or len(big_text.strip()) < 100:
        logger.warning(f"èŠå¤©å†…å®¹è¿‡å°‘ (é•¿åº¦: {len(big_text.strip()) if big_text else 0} å­—ç¬¦)ï¼Œè·³è¿‡ LLM æ€»ç»“ç”Ÿæˆã€‚")
        return False
    
    logger.info(f"è·å–åˆ° {len(history_items)} æ¡ VIP ç›¸å…³æ¶ˆæ¯ï¼Œæ–‡æœ¬é•¿åº¦: {len(big_text)} å­—ç¬¦ï¼Œå¼€å§‹è°ƒç”¨ LLM...")
    
    # è®¡ç®—æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)
    date_str = datetime.datetime.fromtimestamp(start_ms / 1000.0).strftime('%Y-%m-%d')
    
    from utils.local_secrets import ai_model_name
    model = ai_model_name
    
    import concurrent.futures

    # å®šä¹‰å¹¶è¡Œä»»åŠ¡å‡½æ•°
    def call_agent_1():
        logger.info("Agent 1: ç”ŸæˆçŸ¥è¯†åº“ä¸å®è§‚åˆ†æ...")
        prompt_1 = get_knowledge_macro_prompt(date_str) + big_text
        return get_response(prompt_1, model=model)

    def call_agent_2():
        logger.info("Agent 2: ç”Ÿæˆæ ¸å¿ƒè§‚ç‚¹ä¸æ—¶é—´çº¿...")
        prompt_2 = get_timeline_views_prompt(date_str) + big_text
        return get_response(prompt_2, model=model)

    # å¹¶è¡Œæ‰§è¡Œ Agent 1 å’Œ Agent 2
    logger.info("ğŸš€ å¯åŠ¨å¹¶è¡Œä»»åŠ¡: Agent 1 & Agent 2...")
    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        future_1 = executor.submit(call_agent_1)
        future_2 = executor.submit(call_agent_2)
        
        summary_1 = future_1.result()
        summary_2 = future_2.result()
        
    logger.info("âœ… Agent 1 & Agent 2 ä»»åŠ¡å®Œæˆ")

    # åˆå¹¶ç»“æœ
    # ç”¨æˆ·è¦æ±‚ï¼šTimeline (summary_2) åœ¨å‰ï¼ŒKnowledge (summary_1) åœ¨å
    # ä½†æ˜¯ summary_1 åŒ…å«ä¸»æ ‡é¢˜ (# ...)ï¼Œæˆ‘ä»¬éœ€è¦æŠŠæ ‡é¢˜æå–å‡ºæ¥æ”¾åœ¨æœ€å‰é¢
    
    final_summary = ""
    title_line = ""
    body_1 = summary_1
    
    # å°è¯•æå– summary_1 çš„ç¬¬ä¸€è¡Œä½œä¸ºæ ‡é¢˜
    # ä»…å½“ç¬¬ä¸€è¡Œæ˜¯ H1 (# ) æ—¶æ‰æå–ï¼Œé¿å…è¯¯ä¼¤ H2 (## )
    if summary_1 and summary_1.strip().startswith("# "):
        parts = summary_1.strip().split("\n", 1)
        if len(parts) >= 1:
            title_line = parts[0]
            if len(parts) > 1:
                body_1 = parts[1]
            else:
                body_1 = ""
    
    if title_line:
        # ä¸¢å¼ƒåŸæœ‰çš„ title_lineï¼Œåªä¿ç•™ body
        final_summary = f"{summary_2}\n\n{body_1}"
    else:
        # å¦‚æœæå–å¤±è´¥ï¼Œç›´æ¥æ‹¼æ¥ï¼ŒTimeline åœ¨å‰
        final_summary = f"{summary_2}\n\n{summary_1}"

    # ç”ŸæˆåŠ¨æ€å…ƒæ•°æ® (Title & Description)
    logger.info("ç”ŸæˆåŠ¨æ€å…ƒæ•°æ® (Title & Description)...")
    try:
        # è®¡ç®—æ ‡é¢˜æ—¥æœŸ (YYYY.MM.DD)ï¼Œä½¿ç”¨çª—å£å¼€å§‹æ—¶é—´ä½œä¸ºé”šç‚¹ (é€šå¸¸æ˜¯ç¾è‚¡äº¤æ˜“æ—¥å½“å¤©)
        title_date = datetime.datetime.fromtimestamp(start_ms / 1000.0).strftime('%Y.%m.%d')
        
        meta_prompt = get_metadata_prompt(title_date) + f"\n\n{final_summary}"
        meta_response = get_response(meta_prompt, model=model)
        
        # å°è¯•è§£æ JSON
        # æœ‰æ—¶å€™ LLM ä¼šåŒ…è£¹åœ¨ ```json ... ``` ä¸­
        json_str = meta_response
        if "```json" in meta_response:
            match = re.search(r"```json(.*?)```", meta_response, re.DOTALL)
            if match:
                json_str = match.group(1)
        elif "```" in meta_response:
             match = re.search(r"```(.*?)```", meta_response, re.DOTALL)
             if match:
                json_str = match.group(1)
                
        meta_data = json.loads(json_str)
        
        new_title = meta_data.get("title", title)
        new_description = meta_data.get("description", description)
        new_tags = meta_data.get("tags", [])
        
        logger.info(f"åŠ¨æ€å…ƒæ•°æ®ç”ŸæˆæˆåŠŸ: Title='{new_title}', Desc='{new_description}', Tags={new_tags}")
        
        # å°†åŠ¨æ€ç”Ÿæˆçš„ Title åŠ åˆ°æ–‡æ¡£æœ€å‰é¢
        # å…ˆä¿å­˜ body éƒ¨åˆ†
        body_content = final_summary
        
        final_summary = f"# {new_title}\n\n"
        
        # æ˜¾æ€§åŒ–å±•ç¤º Tags (ç”¨æˆ·è¦æ±‚ç§»é™¤)
        # if new_tags:
        #     tags_str = " ".join([f"#{t}" for t in new_tags])
        #     final_summary += f"**Tags:** {tags_str}\n\n"
            
        final_summary += f"{body_content}"
        
    except Exception as e:
        logger.error(f"åŠ¨æ€å…ƒæ•°æ®ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
        new_title = title
        new_description = description
        new_tags = []
        # å¤±è´¥æ—¶ä½¿ç”¨ä¼ å…¥çš„é»˜è®¤ title
        final_summary = f"# {new_title}\n\n{final_summary}"

    # ä¿å­˜åˆ°æ•°æ®åº“
    save_summary_to_db(
        summary=final_summary,
        title=new_title,
        description=new_description,
        model=model,
        raw_chat_text=big_text,
        tags=new_tags,
    )
    
    logger.info("æ€»ç»“ç”Ÿæˆå¹¶ä¿å­˜æˆåŠŸï¼")
    return True

if __name__ == "__main__":
    import sys
    is_force_run = "--force" in sys.argv

    # 1. è®¡ç®—æ—¶é—´çª—å£ (åŒ—äº¬æ—¶é—´ 18:00 - 10:00)
    start_ms, end_ms, title_desc = get_trading_window_cn()
    
    start_str = datetime.datetime.fromtimestamp(start_ms/1000).strftime('%Y-%m-%d %H:%M')
    end_str = datetime.datetime.fromtimestamp(end_ms/1000).strftime('%Y-%m-%d %H:%M')
    logger.info(f"å½“å‰äº¤æ˜“æ—¥çª—å£: {start_str} -> {end_str} ({title_desc})")
    
    # 2. é‡å¤æ€§æ£€æŸ¥ (Deduplication)
    last_time_ms = get_last_summary_time()
    last_time_str = datetime.datetime.fromtimestamp(last_time_ms/1000).strftime('%Y-%m-%d %H:%M:%S')
    
    now_ms = int(time.time() * 1000)
    
    should_run = False
    trigger_reason = ""
    
    # é€»è¾‘ A: å¦‚æœä¸Šæ¬¡æ€»ç»“æ—¶é—´ æ™šäº çª—å£ç»“æŸæ—¶é—´ï¼Œè¯´æ˜è¿™ä¸ªçª—å£çš„æ€»ç»“å·²ç»åšè¿‡äº†
    if last_time_ms >= end_ms:
        if is_force_run:
            logger.warning(f"è¯¥çª—å£æ€»ç»“å·²å­˜åœ¨ (ä¸Šæ¬¡: {last_time_str})ï¼Œä½†æ£€æµ‹åˆ°å¼ºåˆ¶æ‰§è¡Œå‚æ•°ï¼Œç»§ç»­...")
            should_run = True
            trigger_reason = "å¼ºåˆ¶æ‰§è¡Œ (--force)"
        else:
            logger.info(f"è¯¥çª—å£æ€»ç»“å·²å­˜åœ¨ (ä¸Šæ¬¡: {last_time_str} >= çª—å£ç»“æŸ {end_str})ï¼Œè·³è¿‡ä»¥èŠ‚çœ Tokenã€‚")
            exit(0)
            
    # é€»è¾‘ B: å¦‚æœä¸Šæ¬¡æ€»ç»“æ—¶é—´ æ—©äº çª—å£å¼€å§‹æ—¶é—´ï¼Œè¯´æ˜è¿™æ˜¯è¯¥çª—å£çš„ç¬¬ä¸€æ¬¡è¿è¡Œ
    elif last_time_ms < start_ms:
        should_run = True
        trigger_reason = "æ–°äº¤æ˜“æ—¥é¦–æ¬¡è¿è¡Œ"
        
    # é€»è¾‘ C: å¦‚æœä¸Šæ¬¡æ€»ç»“æ—¶é—´ åœ¨ çª—å£ä¸­é—´ (éƒ¨åˆ†æ€»ç»“)
    else:
        # å¦‚æœå½“å‰æ—¶é—´å·²ç»è¿‡äº†çª—å£ç»“æŸæ—¶é—´ (è¡¥å…¨æœ€ç»ˆæŠ¥å‘Š)
        if now_ms > end_ms:
            should_run = True
            trigger_reason = "çª—å£ç»“æŸï¼Œç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"
        else:
            # è¿˜åœ¨çª—å£æœŸå†…ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ–°æ¶ˆæ¯
            # è¿™é‡Œä¸ºäº†èŠ‚çœ Tokenï¼Œæˆ‘ä»¬å¯ä»¥è®¾ç½®ä¸€ä¸ªè¾ƒé«˜çš„é˜ˆå€¼ï¼Œæˆ–è€…ç›´æ¥è·³è¿‡ï¼ˆé™¤é forceï¼‰
            # ç”¨æˆ·è¯´ "å…¶ä»–æ—¶é—´æ²¡æœ‰éœ€æ±‚"ï¼Œæš—ç¤ºåªæœ‰æœ€ç»ˆæŠ¥å‘Šé‡è¦ï¼Ÿ
            # ä½†å¦‚æœç”¨æˆ·åœ¨ 09:00 è¿è¡Œï¼Œå¯èƒ½æƒ³çœ‹æˆªæ­¢ç›®å‰çš„ã€‚
            # ç­–ç•¥ï¼šæ£€æŸ¥æ–°æ¶ˆæ¯æ•°é‡
            logger.info("æ£€æµ‹åˆ°çª—å£å†…å·²æœ‰éƒ¨åˆ†æ€»ç»“ï¼Œæ­£åœ¨æ£€æŸ¥å¢é‡æ¶ˆæ¯...")
            # é¢„æ£€ä¸€ä¸‹æ•°æ®é‡
            new_items, _ = fetch_vip_data(start_ms=last_time_ms, end_ms=end_ms)
            new_count = len(new_items)
            
            if is_force_run:
                should_run = True
                trigger_reason = f"å¼ºåˆ¶æ›´æ–° (å¢é‡ {new_count} æ¡)"
            elif new_count > 50: # åªæœ‰å½“å¢é‡å¤§äº 50 æ¡æ‰æ›´æ–°ï¼Œé¿å…é¢‘ç¹æµªè´¹
                should_run = True
                trigger_reason = f"å¢é‡æ¶ˆæ¯ç§¯ç´¯ ({new_count} > 50)"
            else:
                logger.info(f"å¢é‡æ¶ˆæ¯ä¸è¶³ ({new_count} <= 50)ï¼Œè·³è¿‡ã€‚")
                exit(0)

    if should_run:
        logger.info(f"è§¦å‘æ€»ç»“ç”Ÿæˆï¼ŒåŸå› : {trigger_reason}")
        result = summary_run(start_ms, end_ms, "ç¾è‚¡äº¤æ˜“æ—¥å¤ç›˜", title_desc)
        
        # --force æ¨¡å¼ä¸‹ï¼Œå¦‚æœå½“å‰çª—å£æ²¡æœ‰æ•°æ®ï¼Œå°è¯•å›é€€åˆ°ä¸Šä¸€ä¸ªçª—å£
        if not result and is_force_run:
            logger.info("å½“å‰çª—å£æ— æœ‰æ•ˆæ•°æ®ï¼Œå°è¯•å›é€€åˆ°ä¸Šä¸€ä¸ªäº¤æ˜“æ—¥çª—å£...")
            
            # æœ€å¤šå›é€€ 3 ä¸ªçª—å£ï¼ˆé¿å…æ— é™å¾ªç¯ï¼‰
            for offset in range(1, 4):
                prev_start_ms, prev_end_ms, prev_title_desc = get_trading_window_cn_offset(offset)
                prev_start_str = datetime.datetime.fromtimestamp(prev_start_ms/1000).strftime('%Y-%m-%d %H:%M')
                prev_end_str = datetime.datetime.fromtimestamp(prev_end_ms/1000).strftime('%Y-%m-%d %H:%M')
                logger.info(f"å°è¯•çª—å£ (offset={offset}): {prev_start_str} -> {prev_end_str} ({prev_title_desc})")
                
                result = summary_run(prev_start_ms, prev_end_ms, "ç¾è‚¡äº¤æ˜“æ—¥å¤ç›˜", prev_title_desc)
                if result:
                    logger.info(f"æˆåŠŸä½¿ç”¨çª—å£ (offset={offset}) ç”Ÿæˆæ€»ç»“")
                    break
            else:
                logger.warning("å›é€€ 3 ä¸ªçª—å£åä»æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®")
        elif not result:
            logger.info("ç”±äºæ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œæœ¬æ¬¡æœªç”Ÿæˆæ€»ç»“ã€‚")
    else:
        logger.info("æœªæ»¡è¶³è§¦å‘æ¡ä»¶ï¼Œè·³è¿‡")

