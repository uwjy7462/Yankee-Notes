import json
import os
import time
import random
from typing import Any, Dict, List, Optional, Tuple
from datetime import datetime, timezone
import requests
from loguru import logger
from supabase import create_client, Client
from .local_secrets import whom_headers as headers
from .local_secrets import supabase_url, supabase_key

try:
    supabase: Client = create_client(supabase_url, supabase_key)
except Exception as e:
    logger.error(f"Supabase åˆå§‹åŒ–å¤±è´¥: {e}")
    supabase = None

url = 'https://whop.com/api/graphql/MessagesFetchFeedPosts/'

def _ms_to_iso(ms: Any) -> Optional[str]:
    if ms is not None:
        try:
            return datetime.fromtimestamp(int(ms) / 1000.0, tz=timezone.utc).isoformat()
        except:
            return None
    return None

def _iso_to_ms(iso: Any) -> int:
    if iso:
        try:
            dt = datetime.fromisoformat(str(iso).replace('Z', '+00:00'))
            return int(dt.timestamp() * 1000)
        except:
            return 0
    return 0

def _record_to_post(r: Dict) -> Dict:
    return {
        'id': r['id'],
        'feedId': r['feed_id'],
        'userId': r['user_id'],
        'content': r['content'],
        'richContent': r['rich_content'],
        'replyingToPostId': r['reply_to_post_id'],
        'mentionedUserIds': r['mentioned_user_ids'],
        'attachments': r['attachments'],
        'linkEmbeds': r['link_embeds'],
        'gifs': r['gifs'],
        'reactionCounts': r['reaction_counts'],
        'viewCount': r['view_count'],
        'isPinned': r['is_pinned'],
        'isEdited': r['is_edited'],
        'isDeleted': r['is_deleted'],
        'createdAt': _iso_to_ms(r['posted_at']),
        'updatedAt': _iso_to_ms(r['edited_at']),
        'messageType': 'text'
    }

def _upsert_users_to_db(users_list: List[Dict]) -> None:
    if not users_list or not supabase:
        return
    
    # å…¨å±€å»é‡ï¼šé˜²æ­¢åŒä¸€ä¸ªæ‰¹æ¬¡ä¸­å‡ºç°é‡å¤çš„ user_id å¯¼è‡´ PostgreSQL æŠ¥é”™
    unique_users = {}
    for u in users_list:
        uid = u.get('id')
        if uid:
            unique_users[uid] = u
            
    data = []
    for u in unique_users.values():
        data.append({
            'id': u.get('id'),
            'username': u.get('username'),
            'display_name': u.get('name'),
            'avatar_url': u.get('profilePicSm', {}).get('double') or u.get('profilePicLg', {}).get('double'),
            'roles': u.get('roles'),
            'updated_at': datetime.now(timezone.utc).isoformat()
        })
        
    if data:
        try:
            supabase.table('whop_users').upsert(data).execute()
        except Exception as e:
            logger.error(f"Supabase ç”¨æˆ·å†™å…¥å¤±è´¥: {e}")

def _upsert_posts_to_db(posts: List[Dict]) -> None:
    if not posts or not supabase:
        return
        
    data = []
    for p in posts:
        pid = p.get('id')
        if not pid: continue
        
        try:
            data.append({
                'id': pid,
                'feed_id': p.get('feedId'),
                'user_id': p.get('userId'),
                'content': p.get('content'),
                'rich_content': p.get('richContent'),
                'reply_to_post_id': p.get('replyingToPostId'),
                'mentioned_user_ids': p.get('mentionedUserIds', []),
                'attachments': p.get('attachments', []),
                'link_embeds': p.get('linkEmbeds', []),
                'gifs': p.get('gifs', []),
                'reaction_counts': p.get('reactionCounts', []),
                'view_count': p.get('viewCount', 0),
                'is_pinned': p.get('isPinned', False),
                'is_edited': p.get('isEdited', False),
                'is_deleted': p.get('isDeleted', False),
                'posted_at': _ms_to_iso(p.get('createdAt')),
                'edited_at': _ms_to_iso(p.get('updatedAt')),
                'crawled_at': datetime.now(timezone.utc).isoformat()
            })
        except Exception as e:
            logger.warning(f"è·³è¿‡æ ¼å¼é”™è¯¯çš„å¸–å­ {pid}: {e}")
            continue
            
    if data:
        try:
            supabase.table('whop_posts').upsert(data).execute()
        except Exception as e:
            logger.error(f"Supabase å¸–å­å†™å…¥å¤±è´¥: {e}")

def _fetch_posts_from_db(limit: int, before_ms: Optional[int]) -> List[Dict]:
    if not supabase:
        return []
        
    try:
        query = supabase.table('whop_posts').select('*').order('posted_at', desc=True).limit(limit)
        
        if before_ms:
            before_iso = _ms_to_iso(before_ms)
            if before_iso:
                query = query.lt('posted_at', before_iso)
                
        resp = query.execute()
        return [_record_to_post(r) for r in resp.data]
    except Exception as e:
        logger.error(f"Supabase æŸ¥è¯¢å¤±è´¥: {e}")
        return []

def _get_users_map_from_db(user_ids: List[str]) -> Dict[str, str]:
    if not supabase or not user_ids:
        return {}
        
    try:
        resp = supabase.table('whop_users').select('id, username').in_('id', user_ids).execute()
        return {r['id']: r['username'] for r in resp.data}
    except Exception as e:
        logger.error(f"Supabase ç”¨æˆ·æŸ¥è¯¢å¤±è´¥: {e}")
        return {}

def get_payload(limit: int, before: int = None, feed_id: str = "chat_feed_1CTr5VAdNHtbZAFaTitvoT") -> str:
    if before is not None:
        before_str = str(before)
    else:
        before_str = "null"
        
    return '{"query":"query MessagesFetchFeedPosts($feedType: FeedTypes!, $after: BigInt, $before: BigInt, $aroundId: ID, $feedId: ID!, $includeDeleted: Boolean, $includeReactions: Boolean, $limit: Int, $direction: Direction) {\\n  feedPosts(\\n    feedType: $feedType\\n    after: $after\\n    before: $before\\n    aroundId: $aroundId\\n    feedId: $feedId\\n    includeDeleted: $includeDeleted\\n    includeReactions: $includeReactions\\n    limit: $limit\\n    direction: $direction\\n  ) {\\n    posts {\\n      __typename\\n      ...DmsPostFragment\\n    }\\n    users {\\n      ...BasicUserProfileDetails\\n    }\\n    reactions {\\n      ...ReactionFragment\\n    }\\n  }\\n}\\n\\nfragment DmsPostFragment on DmsPost {\\n  id\\n  createdAt\\n  updatedAt\\n  isDeleted\\n  sortKey\\n  isPosterAdmin\\n  mentionedUserIds\\n  content\\n  feedId\\n  feedType\\n  attachments {\\n    ...Attachment\\n  }\\n  gifs {\\n    height\\n    provider\\n    originalUrl\\n    previewUrl\\n    provider\\n    slug\\n    title\\n    width\\n  }\\n  isEdited\\n  isEveryoneMentioned\\n  isPinned\\n  linkEmbeds {\\n    description\\n    favicon\\n    image\\n    processing\\n    title\\n    url\\n    footer {\\n      title\\n      description\\n      icon\\n    }\\n  }\\n  richContent\\n  userId\\n  viewCount\\n  reactionCounts {\\n    reactionType\\n    userCount\\n    value\\n  }\\n  messageType\\n  embed\\n  replyingToPostId\\n  replyingToPost {\\n    id\\n    richContent\\n    content\\n    gifs {\\n      __typename\\n    }\\n    isDeleted\\n    linkEmbeds {\\n      __typename\\n    }\\n    mentionedUserIds\\n    isEveryoneMentioned\\n    messageType\\n    attachments {\\n      contentType\\n    }\\n    user {\\n      id\\n      name\\n      username\\n      roles\\n      profilePicSm: profileImageSrcset(style: s32) {\\n        double\\n      }\\n    }\\n  }\\n  poll {\\n    options {\\n      id\\n      text\\n    }\\n  }\\n  customAuthor {\\n    displayName\\n    profilePicture {\\n      sourceUrl\\n    }\\n  }\\n}\\n\\nfragment Attachment on AttachmentInterface {\\n  __typename\\n  id\\n  signedId\\n  analyzed\\n  byteSizeV2\\n  filename\\n  contentType\\n  source(variant: original) {\\n    url\\n  }\\n  ... on ImageAttachment {\\n    height\\n    width\\n    blurhash\\n    aspectRatio\\n  }\\n  ... on VideoAttachment {\\n    height\\n    width\\n    duration\\n    aspectRatio\\n    preview(variant: original) {\\n      url\\n    }\\n  }\\n  ... on AudioAttachment {\\n    duration\\n    waveformUrl\\n  }\\n}\\n\\nfragment BasicUserProfileDetails on PublicProfileUser {\\n  id\\n  name\\n  createdAt\\n  bannerImageLg: bannerImageSrcset(style: s600x200) {\\n    double\\n  }\\n  profilePicLg: profileImageSrcset(style: s128) {\\n    double\\n  }\\n  profilePicSm: profileImageSrcset(style: s32) {\\n    double\\n  }\\n  username\\n  createdAt\\n  roles\\n  lastSeenAt\\n  isPlatformPolice\\n}\\n\\nfragment ReactionFragment on Reaction {\\n  id\\n  isDeleted\\n  createdAt\\n  updatedAt\\n  feedId\\n  feedType\\n  postId\\n  postType\\n  userId\\n  reactionType\\n  score\\n  value\\n}","variables":{"feedId":"' + feed_id + '","feedType":"chat_feed","limit":' + str(limit) + ',"before":' + before_str + ',"direction":"desc","includeDeleted":false}}'

def get_universal_payload(limit: int, before_cursor: str = None) -> Dict:
    query = """
    query coreFetchUniversalPosts($feedType: UniversalPostFeedTypes!, $accessPassId: ID, $experienceId: ID, $limit: Int, $beforeCursor: ID, $afterCursor: ID, $appIds: [ID!], $internalOnlyShowGlobalFeed: Boolean) {
  universalPosts(
    feedType: $feedType
    accessPassId: $accessPassId
    experienceId: $experienceId
    limit: $limit
    beforeCursor: $beforeCursor
    afterCursor: $afterCursor
    appIds: $appIds
    internalOnlyShowGlobalFeed: $internalOnlyShowGlobalFeed
  ) {
    universalPosts {
      ...FeedUniversalPost
    }
    beforeCursor
    afterCursor
  }
}
    
    fragment FeedUniversalPost on UniversalPost {
  __typename
  experience {
    id
  }
  app {
    id
  }
  resource {
    __typename
    ... on FeedForumPostUniversalPost {
      forumPost {
        ...UniversalForumPost
      }
    }
    ... on FeedLivestreamFeedUniversalPost {
      livestreamFeed {
        ...UniversalLivestreamFeed
      }
    }
  }
}
    
    fragment UniversalForumPost on ForumPost {
  ...UniversalForumPostContent
  commentUsers(first: 4) {
    nodes {
      id
    }
    totalCount
  }
  comments(first: 2, depth: 1) {
    nodes {
      ...UniversalForumPostContent
    }
    totalCount
  }
}
    
    fragment UniversalForumPostContent on ForumPost {
  id
  createdAt
  title
  content
  richContent
  feedId
  commentCount
  viewCount
  pinned
  reactionCounts {
    reactionType
    userCount
    value
  }
  ownEmojiReactions: ownReactions(first: 1, reactionType: emoji) {
    nodes {
      value
    }
  }
  ownVoteReactions: ownReactions(first: 1, reactionType: vote) {
    nodes {
      value
    }
  }
  gifs {
    originalUrl
    url
    previewUrl
    width
    height
    slug
    title
    provider
  }
  lineItem {
    id
    amount
    redirectUrl
    baseCurrency
  }
  poll {
    options {
      id
      text
    }
  }
  muxAssets {
    id
    status
    signedPlaybackId
    playbackId
    signedThumbnailPlaybackToken
    signedVideoPlaybackToken
    signedStoryboardPlaybackToken
    durationSeconds
  }
  attachments {
    ...Attachment
  }
  userId
  isPosterAdmin
  parentId
  mentionedUserIds
  isDeleted
  isEdited
  user {
    id
    name
    username
    profilePicSm: profileImageSrcset(style: s32) {
        double
    }
  }
}
    
    fragment Attachment on AttachmentInterface {
  __typename
  id
  signedId
  analyzed
  byteSizeV2
  filename
  contentType
  source(variant: original) {
    url
  }
  ... on ImageAttachment {
    height
    width
    blurhash
    aspectRatio
  }
  ... on VideoAttachment {
    height
    width
    duration
    aspectRatio
    preview(variant: original) {
      url
    }
  }
  ... on AudioAttachment {
    duration
    waveformUrl
  }
}
    
    fragment UniversalLivestreamFeed on LivestreamFeed {
  title
  id
  thumbnailUrl
  startedAt
  endedAt
  host {
    id
  }
}
    """
    return {
        "query": query,
        "variables": {
            "appIds": ["app_dYfm2IdXhDMquv"],
            "feedType": "home",
            "limit": limit,
            "experienceId": "exp_JG1I58S5zTHbxs",
            "beforeCursor": before_cursor
        },
        "operationName": "coreFetchUniversalPosts"
    }

def _get_db_min_timestamp_after(after_ms: int) -> int:
    """è·å–æ•°æ®åº“ä¸­åœ¨æŒ‡å®šæ—¶é—´ä¹‹åçš„æœ€æ—©ä¸€æ¡æ¶ˆæ¯çš„æ—¶é—´æˆ³"""
    if not supabase:
        return 0
    try:
        # æŸ¥æ‰¾ posted_at > after_ms çš„æ¶ˆæ¯ï¼ŒæŒ‰ posted_at å‡åºæ’åˆ—ï¼Œå–ç¬¬ä¸€æ¡
        after_iso = _ms_to_iso(after_ms)
        resp = supabase.table('whop_posts')\
            .select('posted_at')\
            .gt('posted_at', after_iso)\
            .order('posted_at', desc=False)\
            .limit(1)\
            .execute()
            
        if resp.data:
            return _iso_to_ms(resp.data[0]['posted_at'])
    except Exception as e:
        logger.error(f"æŸ¥è¯¢ DB Min Timestamp å¤±è´¥: {e}")
    return 0

def get_latest_db_timestamp() -> int:
    """è·å–æ•°æ®åº“ä¸­æœ€æ–°ä¸€æ¡æ¶ˆæ¯çš„æ—¶é—´æˆ³"""
    if not supabase:
        return 0
    try:
        resp = supabase.table('whop_posts')\
            .select('posted_at')\
            .order('posted_at', desc=True)\
            .limit(1)\
            .execute()
            
        if resp.data:
            return _iso_to_ms(resp.data[0]['posted_at'])
    except Exception as e:
        logger.error(f"æŸ¥è¯¢ DB Latest Timestamp å¤±è´¥: {e}")
    return 0

def get_latest_universal_db_timestamp() -> int:
    """è·å–æ•°æ®åº“ä¸­æœ€æ–°ä¸€æ¡ Universal Post çš„æ—¶é—´æˆ³"""
    if not supabase:
        return 0
    try:
        resp = supabase.table('whop_universal_posts')\
            .select('posted_at')\
            .order('posted_at', desc=True)\
            .limit(1)\
            .execute()
            
        if resp.data:
            return _iso_to_ms(resp.data[0]['posted_at'])
    except Exception as e:
        logger.error(f"æŸ¥è¯¢ Universal DB Latest Timestamp å¤±è´¥: {e}")
    return 0

def _upsert_universal_posts_to_db(posts: List[Dict]) -> None:
    if not posts or not supabase:
        return
        
    data = []
    users_to_upsert = []
    
    for p_item in posts:
        resource = p_item.get('resource', {})
        p = resource.get('forumPost') or resource.get('forum_post')
        if not p: continue
        
        pid = p.get('id')
        if not pid: continue
        
        # Extract user info for whop_users table
        user_data = p.get('user')
        if user_data:
            users_to_upsert.append(user_data)
        
        try:
            data.append({
                'id': pid,
                'title': p.get('title'),
                'content': p.get('content'),
                'rich_content': p.get('richContent'),
                'feed_id': p.get('feedId'),
                'user_id': p.get('userId'),
                'comment_count': p.get('commentCount', 0),
                'view_count': p.get('viewCount', 0),
                'is_pinned': p.get('pinned', False),
                'reaction_counts': p.get('reactionCounts', []),
                'attachments': p.get('attachments', []),
                'mentioned_user_ids': p.get('mentionedUserIds', []),
                'is_deleted': p.get('isDeleted', False),
                'is_edited': p.get('isEdited', False),
                'posted_at': _ms_to_iso(p.get('createdAt')),
                'crawled_at': datetime.now(timezone.utc).isoformat()
            })
        except Exception as e:
            logger.warning(f"è·³è¿‡æ ¼å¼é”™è¯¯çš„ Universal Post {pid}: {e}")
            continue
            
    if users_to_upsert:
        _upsert_users_to_db(users_to_upsert)
        
    if data:
        try:
            supabase.table('whop_universal_posts').upsert(data).execute()
        except Exception as e:
            logger.error(f"Supabase Universal Post å†™å…¥å¤±è´¥: {e}")

def get_universal_posts(
    limit: int, 
    before_cursor: Optional[str] = None, 
    stop_at_timestamp: Optional[int] = None,
    max_api_requests: int = 20,
    accumulate_results: bool = True
) -> List[Dict]:
    logger.info(f"å‡†å¤‡è·å– Universal å†å²æ¶ˆæ¯ï¼Œlimit={limit}ï¼Œbefore_cursor={before_cursor}, stop_at={stop_at_timestamp}, max_req={max_api_requests}")
    
    history_items = []
    seen_ids = set()
    request_count = 0
    
    # Anti-Ban: Coffee Break Logic
    requests_since_break = 0
    next_break_threshold = random.randint(20, 40)
    
    next_before_cursor = before_cursor
    session_start_ts = None
    total_saved_count = 0
    
    target_str = datetime.fromtimestamp(stop_at_timestamp/1000.0).strftime('%Y-%m-%d %H:%M:%S') if stop_at_timestamp else 'æ— é™åˆ¶'
    logger.info(f"ğŸš€ å¼€å§‹åŒæ­¥ Universal ä»»åŠ¡... (ç›®æ ‡: {target_str})")

    universal_url = "https://whop.com/api/graphql/coreFetchUniversalPosts/"
    
    while len(history_items) < limit:
        if request_count >= max_api_requests:
            logger.warning(f"è¾¾åˆ° API è¯·æ±‚æ¬¡æ•°ä¸Šé™ ({max_api_requests})ï¼Œåœæ­¢ API æ‹‰å–")
            break
            
        remaining = limit - len(history_items)
        page_limit = min(50, remaining)
        
        try:
            max_retries = 3
            for retry_attempt in range(max_retries):
                try:
                    payload = get_universal_payload(page_limit, next_before_cursor)
                    resp = requests.post(universal_url, headers=headers, json=payload, timeout=30)
                    
                    if resp.status_code == 429:
                        logger.critical("âš ï¸ è§¦å‘ API é™æµ (429)ï¼å¼ºåˆ¶ä¼‘çœ  10 åˆ†é’Ÿ...")
                        time.sleep(600)
                        raise Exception("API Rate Limit Hit (429) - Safety Stop")

                    resp.raise_for_status()
                    request_count += 1
                    requests_since_break += 1
                    break
                except Exception as e:
                    if retry_attempt < max_retries - 1:
                        wait_time = (retry_attempt + 1) * 5
                        logger.warning(f"APIè¯·æ±‚å¤±è´¥ ({e})ï¼Œæ­£åœ¨é‡è¯• ({retry_attempt + 1}/{max_retries})ï¼Œç­‰å¾… {wait_time} ç§’...")
                        time.sleep(wait_time)
                    else:
                        raise e
            
            resp_json = resp.json()
            if 'errors' in resp_json:
                logger.error(f"GraphQL é”™è¯¯: {resp_json['errors']}")
                break
                
            data = resp_json.get('data', {}).get('universalPosts', {})
            posts_page = data.get('universalPosts', [])
            next_before_cursor = data.get('beforeCursor')
            
            if not posts_page:
                logger.warning("APIæœªè¿”å›æ›´å¤š Universal æ¶ˆæ¯")
                break
                
            _upsert_universal_posts_to_db(posts_page)
            
            stop_fetch_signal = False
            for p_item in posts_page:
                resource = p_item.get('resource', {})
                post = resource.get('forumPost') or resource.get('forum_post')
                if not post: continue
                
                pid = post.get('id')
                created = int(post.get('createdAt', 0))
                
                if session_start_ts is None:
                    session_start_ts = created
                
                if stop_at_timestamp and created < stop_at_timestamp:
                    t_created = datetime.fromtimestamp(created/1000.0).strftime('%Y-%m-%d %H:%M:%S')
                    t_stop = datetime.fromtimestamp(stop_at_timestamp/1000.0).strftime('%Y-%m-%d %H:%M:%S')
                    logger.success(f"âœ… ä»»åŠ¡ç›®æ ‡è¾¾æˆï¼å½“å‰æ¶ˆæ¯æ—¶é—´ ({t_created}) å·²æ—©äºè®¾å®šç›®æ ‡ ({t_stop})ï¼Œåœæ­¢æ‹‰å–ã€‚")
                    stop_fetch_signal = True
                    break
                
                if pid not in seen_ids:
                    if accumulate_results:
                        history_items.append(p_item)
                    else:
                        history_items.append({'id': pid, 'createdAt': created})
                    total_saved_count += 1
                    seen_ids.add(pid)
                
                if len(history_items) >= limit:
                    stop_fetch_signal = True
                    break
            
            # Progress Dashboard
            if (request_count % 5 == 0 or request_count == 1):
                try:
                    last_created = int((posts_page[-1].get('resource', {}).get('forumPost') or {}).get('createdAt', 0))
                    progress_pct = 0.0
                    if stop_at_timestamp and session_start_ts and session_start_ts > stop_at_timestamp:
                        total_range = session_start_ts - stop_at_timestamp
                        current_progress = session_start_ts - last_created
                        progress_pct = max(0.0, min(100.0, (current_progress / total_range) * 100))
                    
                    bar = 'â–“' * int(20 * progress_pct / 100) + 'â–‘' * (20 - int(20 * progress_pct / 100))
                    logger.info(f"\n [Universal åŒæ­¥è¿›åº¦] {progress_pct:.1f}% {bar}\n ğŸ“Š ç´¯è®¡ä¿å­˜: {total_saved_count} æ¡ | è¯·æ±‚: {request_count} æ¬¡")
                except: pass

            if stop_fetch_signal or not next_before_cursor:
                break
                
            # Anti-Ban
            if requests_since_break >= next_break_threshold:
                break_duration = random.randint(60, 180)
                logger.info(f"â˜• Coffee Break: {break_duration}s...")
                time.sleep(break_duration)
                requests_since_break = 0
                next_break_threshold = random.randint(20, 40)
            else:
                time.sleep(random.uniform(4, 8))
            
        except Exception as e:
            logger.error(f"Universal APIè¯·æ±‚å¤±è´¥: {e}")
            break
            
    return history_items


def get_history_posts(
    limit: int, 
    before: Optional[int] = None, 
    is_whole_day: bool = False,
    stop_at_timestamp: Optional[int] = None,
    max_api_requests: int = 20,
    accumulate_results: bool = True,
    feed_id: str = "chat_feed_1CTr5VAdNHtbZAFaTitvoT",
    allowed_usernames: Optional[List[str]] = None
) -> Tuple[List[Dict], Dict[str, str]]:
    logger.info(f"å‡†å¤‡è·å–å†å²æ¶ˆæ¯ [{feed_id}]ï¼Œlimit={limit}ï¼Œbefore={before}, stop_at={stop_at_timestamp}, max_req={max_api_requests}, allowed_users={allowed_usernames}")
    
    history_items = []
    seen_ids = set()
    
    should_fetch_api = True
    should_fetch_api = True
    request_count = 0
    
    # Anti-Ban: Coffee Break Logic
    requests_since_break = 0
    next_break_threshold = random.randint(20, 40)
    
    next_before = before
    session_start_ts = None # ç”¨äºè®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
    total_saved_count = 0 # ç”¨äºæ˜¾ç¤ºç´¯è®¡ä¿å­˜æ•°é‡
    
    target_str = datetime.fromtimestamp(stop_at_timestamp/1000.0).strftime('%Y-%m-%d %H:%M:%S') if stop_at_timestamp else 'æ— é™åˆ¶'
    logger.info(f"ğŸš€ å¼€å§‹åŒæ­¥ä»»åŠ¡... (ç›®æ ‡: {target_str})")

    
    while len(history_items) < limit and should_fetch_api:
        # 1. å®‰å…¨æ£€æŸ¥
        if request_count >= max_api_requests:
            logger.warning(f"è¾¾åˆ° API è¯·æ±‚æ¬¡æ•°ä¸Šé™ ({max_api_requests})ï¼Œåœæ­¢ API æ‹‰å–")
            break
            
        remaining = limit - len(history_items)
        # æ¨¡ä»¿æµè§ˆå™¨è¡Œä¸ºï¼Œä½¿ç”¨ 51 ä½œä¸ºåˆ†é¡µå¤§å°
        page_limit = min(51, remaining)
        
        # è½¬æ¢ next_before ä¸ºå¯è¯»æ—¶é—´
        next_before_str = "Latest"
        if next_before:
            try:
                next_before_str = datetime.fromtimestamp(next_before / 1000.0).strftime('%Y-%m-%d %H:%M:%S')
            except:
                next_before_str = str(next_before)
        
        # å‡å°‘æ—¥å¿—å™ªéŸ³ï¼šç§»é™¤è¯·æ±‚å‰çš„æ—¥å¿—ï¼Œæ”¹ä¸ºè¯·æ±‚åæ±‡æ€»
        # if request_count % 5 == 0:
        #    logger.info(f"è¯·æ±‚APIè·å–æ¶ˆæ¯ (ç¬¬ {request_count + 1} æ¬¡)ï¼Œpage_limit={page_limit}ï¼Œå½“å‰è¿›åº¦: {next_before_str}")
        
        # Retry mechanism for API requests
        try:
            max_retries = 3
            for retry_attempt in range(max_retries):
                try:
                    payload = get_payload(page_limit, next_before, feed_id=feed_id)
                    resp = requests.request('POST', url, headers=headers, data=payload, timeout=30)
                    
                    if resp.status_code == 429:
                        logger.critical("âš ï¸ è§¦å‘ API é™æµ (429 Too Many Requests)ï¼")
                        logger.critical("ä¸ºäº†è´¦å·å®‰å…¨ï¼Œè„šæœ¬å°†å¼ºåˆ¶ä¼‘çœ  10 åˆ†é’Ÿ...")
                        time.sleep(600)
                        # ä¼‘çœ åæŠ›å‡ºå¼‚å¸¸ç»“æŸæœ¬æ¬¡è¿è¡Œï¼Œäººå·¥æ£€æŸ¥æ›´å®‰å…¨
                        raise Exception("API Rate Limit Hit (429) - Safety Stop")

                    resp.raise_for_status()
                    request_count += 1
                    requests_since_break += 1
                    break # Success, exit retry loop
                except (requests.exceptions.RequestException, requests.exceptions.SSLError) as e:
                    if "429" in str(e):
                         # Double check if 429 was caught as exception
                        logger.critical("âš ï¸ è§¦å‘ API é™æµ (429)ï¼å¼ºåˆ¶ä¼‘çœ  10 åˆ†é’Ÿ...")
                        time.sleep(600)
                        raise e

                    if retry_attempt < max_retries - 1:
                        wait_time = (retry_attempt + 1) * 5 # 5s, 10s, 15s
                        logger.warning(f"APIè¯·æ±‚å¤±è´¥ ({e})ï¼Œæ­£åœ¨é‡è¯• ({retry_attempt + 1}/{max_retries})ï¼Œç­‰å¾… {wait_time} ç§’...")
                        time.sleep(wait_time)
                    else:
                        logger.error(f"APIè¯·æ±‚å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
                        raise e # Re-raise to be caught by outer try-except
            
            try:
                json_response = resp.json()
                if 'errors' in json_response:
                    logger.error(f"GraphQL Errors: {json_response['errors']}")
                data = json_response['data']['feedPosts']
            except Exception as e:
                logger.error(f"è§£æå“åº”å¤±è´¥: {e}, Response: {resp.text[:500]}")
                raise e
            user_json = data['users']
            posts_page = data['posts']
            
            if not posts_page:
                logger.warning("APIæœªè¿”å›æ›´å¤šæ¶ˆæ¯")
                break
                
            _upsert_users_to_db(user_json)
            
            # Filter posts if allowed_usernames is set
            filtered_posts_page = []
            if allowed_usernames:
                # Create a map of user_id -> username
                uid_to_username = {}
                for u in user_json:
                    uid = u.get('id')
                    uname = u.get('username')
                    if uid and uname:
                        uid_to_username[uid] = uname
                
                for post in posts_page:
                    uid = post.get('userId')
                    username = uid_to_username.get(uid)
                    if username and username in allowed_usernames:
                        filtered_posts_page.append(post)
                    else:
                        # Optional: Log filtered out posts
                        # logger.debug(f"Filtered out post from user {username} ({uid})")
                        pass
            else:
                filtered_posts_page = posts_page

            _upsert_posts_to_db(filtered_posts_page)
            
            min_created_this_page = None
            stop_fetch_signal = False
            
            for post in posts_page:
                pid = post.get('id')
                if not pid: continue
                
                created = int(post.get('createdAt', 0))
                
                if min_created_this_page is None or created < min_created_this_page:
                    min_created_this_page = created
                
                if before is not None and created >= before:
                    continue
                    
                if pid in seen_ids:
                    continue
                
                if stop_at_timestamp and created < stop_at_timestamp:
                    t_created = datetime.fromtimestamp(created/1000.0).strftime('%Y-%m-%d %H:%M:%S')
                    t_stop = datetime.fromtimestamp(stop_at_timestamp/1000.0).strftime('%Y-%m-%d %H:%M:%S')
                    logger.success(f"âœ… ä»»åŠ¡ç›®æ ‡è¾¾æˆï¼å½“å‰æ¶ˆæ¯æ—¶é—´ ({t_created}) å·²æ—©äºè®¾å®šç›®æ ‡ ({t_stop})ï¼Œåœæ­¢æ‹‰å–ã€‚")
                    stop_fetch_signal = True
                    break
                
                if accumulate_results:
                    history_items.append(post)
                # å³ä½¿ä¸ç´¯ç§¯ï¼Œä¹Ÿéœ€è¦è®°å½• ID ä»¥é¿å…é‡å¤å¤„ç†ï¼ˆå¦‚æœ limit è¾ƒå°ï¼‰
                # ä½†å¯¹äºå¤§æ•°æ®é‡åŒæ­¥ï¼Œseen_ids ä¹Ÿå¯èƒ½è¿‡å¤§ï¼Œè¿™é‡Œæš‚ä¸”ä¿ç•™ï¼Œ
                # å› ä¸º seen_ids ä»…å­˜ ID å­—ç¬¦ä¸²ï¼Œç™¾ä¸‡çº§ä¹Ÿå°±å‡ å MBï¼Œå¯æ¥å—ã€‚
                
                # å¦‚æœä¸ç´¯ç§¯ï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨ç»´æŠ¤ä¸€ä¸ªè®¡æ•°å™¨æ¥åˆ¤æ–­æ˜¯å¦è¾¾åˆ° limit
                # ä½†å¤–å±‚å¾ªç¯æ˜¯ç”¨ len(history_items) åˆ¤æ–­çš„ã€‚
                # ä¿®æ­£ï¼šå¦‚æœä¸ç´¯ç§¯ï¼Œæˆ‘ä»¬æ— æ³•ç”¨ len(history_items) å‡†ç¡®æ§åˆ¶ limitã€‚
                # ä½†é€šå¸¸ä¸ç´¯ç§¯æ¨¡å¼ä¸‹ï¼Œlimit éƒ½æ˜¯è®¾å¾—æå¤§ï¼Œä¸»è¦é  stop_at åœæ­¢ã€‚
                # ä¸ºäº†å…¼å®¹ï¼Œå¦‚æœ accumulate_results=Falseï¼Œæˆ‘ä»¬å¾€ history_items æ”¾ä¸€ä¸ªç©ºå ä½ç¬¦æˆ–è€…ä»…æ”¾ IDï¼Ÿ
                # ä¸ï¼Œä¸ºäº†å†…å­˜æœ€ä¼˜åŒ–ï¼Œæˆ‘ä»¬æœ€å¥½æ”¹ç”¨ total_processed_count è®¡æ•°ã€‚
                # ç®€å•èµ·è§ï¼Œå¦‚æœ accumulate_results=Falseï¼Œæˆ‘ä»¬å°±ä¸ append åˆ° history_itemsï¼Œ
                # ä½†æ˜¯ä¸ºäº†è®© `while len(history_items) < limit` å¾ªç¯ç»§ç»­ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ç§æ–¹å¼ã€‚
                # æ–¹æ¡ˆï¼šå¼•å…¥ total_fetched å˜é‡ã€‚
                
                # ä¿®æ­£é€»è¾‘ï¼š
                # 1. å§‹ç»ˆç»´æŠ¤ seen_ids
                # 2. å§‹ç»ˆç»´æŠ¤ total_fetched (åœ¨å¤–éƒ¨) -> å®é™…ä¸Š len(history_items) å°±æ˜¯ã€‚
                # å¦‚æœ accumulate_results=Falseï¼Œæˆ‘ä»¬å°±ä¸ append full objectã€‚
                # æˆ‘ä»¬å¯ä»¥ append ä¸€ä¸ªæå°çš„å ä½ç¬¦ï¼Œæ¯”å¦‚ 1ã€‚
                if accumulate_results:
                    history_items.append(post)
                else:
                    # ä»…è®¡æ•°ï¼Œä¸ºäº†è®©å¾ªç¯æ¡ä»¶ len(history_items) < limit æ­£å¸¸å·¥ä½œ
                    # åŒæ—¶ä¸ºäº†æœ€åèƒ½è¿”å›ç‚¹ä¸œè¥¿ï¼ˆè™½ç„¶æ²¡ç”¨ï¼‰ï¼Œappend ä¸€ä¸ªè½»é‡çº§å¯¹è±¡
                    history_items.append({'id': pid, 'createdAt': created}) # æœ€å°åŒ–å­˜å‚¨
                    total_saved_count += 1
                
                seen_ids.add(pid)
                
                if len(history_items) >= limit:
                    stop_fetch_signal = True
                    break
            
            # --- Smart Jump æ ¸å¿ƒé€»è¾‘ (Gap Detection Strategy) ---
            if not stop_fetch_signal and min_created_this_page and stop_at_timestamp:
                try:
                    # æ£€æŸ¥ min_created_this_page (æœ¬é¡µæœ€è€) æ˜¯å¦åœ¨ DB ä¸­æœ‰â€œæ¥ç»­â€
                    # å®šä¹‰â€œæ¥ç»­â€çš„é˜ˆå€¼ï¼šæ¯”å¦‚ 12 å°æ—¶å†…æœ‰æ¶ˆæ¯
                    CHECK_RANGE_HOURS = 12
                    check_limit_ts = min_created_this_page - (CHECK_RANGE_HOURS * 3600 * 1000)
                    
                    if check_limit_ts < stop_at_timestamp:
                        check_limit_ts = stop_at_timestamp
                        
                    # æŸ¥è¯¢ DB: åœ¨ (check_limit_ts, min_created_this_page) èŒƒå›´å†…æ˜¯å¦æœ‰æ¶ˆæ¯ï¼Ÿ
                    # ç­–ç•¥ï¼š
                    # 1. ä» min_created_this_page å¼€å§‹ï¼Œå‘åï¼ˆæ›´æ—§ï¼‰æ‰«æ DBã€‚
                    # 2. å¯»æ‰¾ç¬¬ä¸€ä¸ªâ€œæ—¶é—´æ–­å±‚â€ï¼ˆGapï¼‰ã€‚
                    #    Gap å®šä¹‰ï¼šä¸¤æ¡ç›¸é‚»æ¶ˆæ¯çš„æ—¶é—´å·® > GAP_THRESHOLD (ä¾‹å¦‚ 6 å°æ—¶)
                    # 3. å¦‚æœæ‰¾åˆ° Gapï¼Œè·³åˆ° Gap çš„å¼€å§‹æ—¶é—´ï¼ˆå³è¾ƒæ™šçš„é‚£æ¡æ¶ˆæ¯çš„æ—¶é—´ï¼‰ã€‚
                    # 4. å¦‚æœæ²¡æ‰¾åˆ° Gapï¼ˆæ‰«æäº†ä¸€å®šèŒƒå›´ï¼‰ï¼Œè¯´æ˜è¿™ä¸€æ®µéƒ½å¾ˆå¯†é›†ï¼Œç›´æ¥è·³åˆ°æ‰«æåˆ°çš„æœ€è¿œç«¯ã€‚
                    
                    GAP_THRESHOLD_MS = 6 * 3600 * 1000  # 6 å°æ—¶
                    SCAN_LIMIT = 5000 # æé«˜æ‰«ææ•ˆç‡ï¼šæ¯æ¬¡æ‰«æ 5000 æ¡ DB è®°å½•
                    
                    # è·å–ä¸€æ‰¹æ›´æ—§çš„æ¶ˆæ¯çš„æ—¶é—´æˆ³
                    scan_query = supabase.table('whop_posts')\
                        .select('posted_at')\
                        .lt('posted_at', _ms_to_iso(min_created_this_page))\
                        .gt('posted_at', _ms_to_iso(stop_at_timestamp))\
                        .order('posted_at', desc=True)\
                        .limit(SCAN_LIMIT)
                        
                    scan_resp = scan_query.execute()
                    scan_data = scan_resp.data
                    
                    if scan_data:
                        # æ‰¾åˆ°äº†æ›´æ—§çš„æ•°æ®ï¼Œå¼€å§‹å¯»æ‰¾ Gap
                        jump_target_ts = None
                        last_ts = min_created_this_page
                        
                        for row in scan_data:
                            curr_ts = _iso_to_ms(row['posted_at'])
                            delta = last_ts - curr_ts
                            
                            if delta > GAP_THRESHOLD_MS:
                                # å‘ç°æ–­å±‚ï¼
                                jump_target_ts = last_ts
                                t_readable = datetime.fromtimestamp(jump_target_ts/1000.0).strftime('%Y-%m-%d %H:%M:%S')
                                logger.info(f"Smart Jump: å‘ç°æ—¶é—´æ–­å±‚ ({delta/3600000:.1f}h)ï¼Œå‡†å¤‡è·³è·ƒåˆ°æ–­å±‚è¾¹ç¼˜: {t_readable}")
                                break
                            
                            last_ts = curr_ts
                            
                        if jump_target_ts:
                            min_created_this_page = jump_target_ts
                        else:
                            # æ²¡å‘ç°æ–­å±‚ï¼Œè¯´æ˜è¿™ 5000 æ¡æ•°æ®æ˜¯è¿ç»­çš„ã€‚
                            deepest_ts = _iso_to_ms(scan_data[-1]['posted_at'])
                            t_readable = datetime.fromtimestamp(deepest_ts/1000.0).strftime('%Y-%m-%d %H:%M:%S')
                            logger.info(f"Smart Jump: æœªå‘ç°æ–­å±‚ï¼Œå®‰å…¨è·³è¿‡ {len(scan_data)} æ¡æœ¬åœ°æ•°æ®ï¼Œåˆ°è¾¾: {t_readable}")
                            min_created_this_page = deepest_ts
                            
                except Exception as e:
                    logger.error(f"Smart Jump Check Failed: {e}")

            # --- Log Progress Dashboard ---
            if posts_page:
                # Update session start on first batch
                p_newest = int(posts_page[0].get('createdAt', 0))
                if session_start_ts is None:
                    session_start_ts = p_newest

                if (request_count % 5 == 0 or request_count == 1):
                    try:
                        p_oldest = int(posts_page[-1].get('createdAt', 0))
                        
                        # Calculate Progress
                        progress_pct = 0.0
                        if stop_at_timestamp and session_start_ts and session_start_ts > stop_at_timestamp:
                            total_range = session_start_ts - stop_at_timestamp
                            current_progress = session_start_ts - p_oldest
                            progress_pct = (current_progress / total_range) * 100
                            progress_pct = max(0.0, min(100.0, progress_pct))
                        
                        # Progress Bar
                        bar_len = 20
                        filled_len = int(bar_len * progress_pct / 100)
                        bar = 'â–“' * filled_len + 'â–‘' * (bar_len - filled_len)
                        
                        fmt = '%Y-%m-%d %H:%M'
                        t_current = datetime.fromtimestamp(p_oldest/1000.0).strftime(fmt)
                        t_target = datetime.fromtimestamp(stop_at_timestamp/1000.0).strftime(fmt) if stop_at_timestamp else "Inf"
                        
                        break_countdown = next_break_threshold - requests_since_break
                        
                        msg = (
                            f"\n{'='*50}\n"
                            f" [åŒæ­¥è¿›åº¦] {progress_pct:.1f}% {bar}\n"
                            f" ğŸ“… å½“å‰ä½ç½®: {t_current}  -->  ç›®æ ‡: {t_target}\n"
                            f" ğŸ“Š ç´¯è®¡ä¿å­˜: {total_saved_count} æ¡ | æœ¬æ¬¡è¯·æ±‚: {request_count} æ¬¡\n"
                            f" â˜• çŠ¶æ€: æ­£å¸¸æŠ“å–ä¸­ (å†è¿‡ {break_countdown} æ¬¡è¯·æ±‚ä¼‘æ¯)\n"
                            f"{'='*50}"
                        )
                        logger.info(msg)
                    except Exception as e:
                        logger.warning(f"æ—¥å¿—æ‰“å°å‡ºé”™: {e}")


            if stop_fetch_signal:
                break
                
            next_before = min_created_this_page
            if next_before is None:
                break
                
            # --- Anti-Ban: Random Delay & Coffee Break ---
            
            # 1. Check Coffee Break
            if requests_since_break >= next_break_threshold:
                break_duration = random.randint(60, 180) # 1-3 minutes
                logger.info(f"â˜• å–å’–å•¡æ—¶é—´ (Coffee Break): å·²è¿ç»­è¯·æ±‚ {requests_since_break} æ¬¡ï¼Œä¼‘æ¯ {break_duration} ç§’...")
                time.sleep(break_duration)
                requests_since_break = 0
                next_break_threshold = random.randint(20, 40) # Reset threshold
            else:
                # 2. Normal Random Delay (Increased for safety)
                sleep_time = random.uniform(4, 8) # 4-8 seconds
                time.sleep(sleep_time)
            
        except Exception as e:
            logger.error(f"APIè¯·æ±‚å¤±è´¥: {e}")
            break
    
    # è¡¥é½é€»è¾‘ (Smart Jump æ¨¡å¼ä¸‹é€šå¸¸ä¸éœ€è¦è¡¥é½ï¼Œå› ä¸ºæˆ‘ä»¬æ˜¯è·³è·ƒå¼æ‹‰å–)
    # ä½†å¦‚æœæœ€åä¸€æ®µæ˜¯åœ¨ DB é‡Œï¼Œæˆ‘ä»¬è·³åˆ°äº† stop_atï¼Œå¾ªç¯ç»“æŸã€‚
    # å¦‚æœæœ€åä¸€æ®µä¸åœ¨ DB é‡Œï¼Œæˆ‘ä»¬æ‹‰åˆ°äº† stop_atï¼Œå¾ªç¯ç»“æŸã€‚
    # æ‰€ä»¥è¿™é‡Œä¸éœ€è¦é¢å¤–çš„ DB è¡¥é½é€»è¾‘ï¼Œé™¤éæ˜¯ä¸ºäº†æ»¡è¶³ limit (ä½†ç°åœ¨ limit å¾ˆå¤§)ã€‚
    # ä¸ºäº†ä¿æŒå…¼å®¹æ€§ï¼Œæˆ‘ä»¬ä¿ç•™ç®€å•çš„æ’åºè¿”å›ã€‚
            
    history_items = sorted(history_items, key=lambda p: int(p.get('createdAt', 0)), reverse=True)
    
    user_ids = {p.get('userId') for p in history_items if p.get('userId')}
    users_cache = _get_users_map_from_db(list(user_ids))
    
    if is_whole_day:
        # è¿™ä¸ªé€»è¾‘åœ¨ Smart Sync ä¸‹å¯èƒ½ä¸å¤ªé€‚ç”¨ï¼Œä½†ä¿ç•™
        pass
        
    logger.info(f"æœ€ç»ˆè¿”å›æ¶ˆæ¯æ•°é‡ï¼š{len(history_items)}")
    return history_items, users_cache
